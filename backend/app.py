import os
import sys
import streamlit as st

# 1. æ¶ˆé™¤ Tokenizers è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# 2. å¼ºåˆ¶å›½å†…é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# é€‚é… LangChain v1.2 çš„å¼•ç”¨
try:
    from langchain_classic.chains import create_retrieval_chain
    from langchain_classic.chains.combine_documents import create_stuff_documents_chain
except ImportError:
    print("âŒ é”™è¯¯: æœªæ‰¾åˆ° langchain-classic åŒ…ã€‚")
    print("ğŸ‘‰ è¯·è¿è¡Œ: pip install langchain-classic")
    sys.exit(1)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv("../.env")

# ================= é…ç½® =================
current_dir = os.path.dirname(os.path.abspath(__file__))
PERSIST_DIRECTORY = os.path.join(current_dir, "chroma_db")
EMBEDDING_MODEL_NAME = "BAAI/bge-small-zh-v1.5"
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

# ================= é¡µé¢è®¾ç½® =================
st.set_page_config(page_title="Design Copilot", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ¤– Design Copilot (RAG)")

# ================= æ ¸å¿ƒé€»è¾‘ (å¸¦ç¼“å­˜) =================
@st.cache_resource
def load_chain():
    """
    åŠ è½½æ¨¡å‹å’Œæ•°æ®åº“ã€‚
    ä½¿ç”¨ cache_resource è£…é¥°å™¨ï¼Œç¡®ä¿åªåŠ è½½ä¸€æ¬¡ï¼Œ
    é˜²æ­¢æ¯æ¬¡å‘æ¶ˆæ¯éƒ½é‡æ–°åŠ è½½æ¨¡å‹ã€‚
    """
    print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– RAG é“¾...")
    
    # 1. åŠ è½½ Embedding
    embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    # 2. è¿æ¥ Chroma
    if not os.path.exists(PERSIST_DIRECTORY):
        st.error(f"æ‰¾ä¸åˆ°æ•°æ®åº“: {PERSIST_DIRECTORY}")
        return None
        
    vector_db = Chroma(
        persist_directory=PERSIST_DIRECTORY,
        embedding_function=embedding
    )
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    
    # 3. åŠ è½½ LLM
    llm = ChatOpenAI(
        model="deepseek-chat", 
        api_key=DEEPSEEK_API_KEY, 
        base_url="https://api.deepseek.com",
        temperature=0.1
    )
    
    # 4. Prompt
    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„å‰ç«¯å¼€å‘ä¸“å®¶ (Design Copilot)ã€‚
    è¯·æ ¹æ®ä»¥ä¸‹ <context> æ ‡ç­¾ä¸­çš„å‚è€ƒæ–‡æ¡£ï¼Œå›ç­”ç”¨æˆ·çš„ <input>ã€‚
    
    <context>
    {context}
    </context>
    
    <input>
    {input}
    </input>

    ã€è¦æ±‚ã€‘ï¼š
    1. ä¼˜å…ˆä½¿ç”¨å‚è€ƒæ–‡æ¡£ä¸­çš„ç»„ä»¶ APIã€‚
    2. ç›´æ¥ç»™å‡ºå®Œæ•´çš„ã€å¯è¿è¡Œçš„ä»£ç ã€‚
    3. å¦‚æœæ–‡æ¡£æœªæåŠï¼Œè¯·è¯´æ˜ã€‚
    """)
    
    # 5. æ„å»ºé“¾
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    
    print("âœ… RAG é“¾åˆå§‹åŒ–å®Œæˆ")
    return rag_chain

# åŠ è½½é“¾ (åªä¼šè¿è¡Œä¸€æ¬¡)
chain = load_chain()

# ================= èŠå¤©ç•Œé¢é€»è¾‘ =================

# 1. åˆå§‹åŒ–èŠå¤©å†å² (Session State)
if "messages" not in st.session_state:
    st.session_state.messages = []

# 2. æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 3. å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„éœ€æ±‚ (ä¾‹å¦‚: ç»™æˆ‘ä¸€ä¸ªå¸¦å›¾æ ‡çš„æŒ‰é’®)"):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    # è®°å½•åˆ°å†å²
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ç”Ÿæˆå›ç­”
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("â³ æ­£åœ¨æ€è€ƒå¹¶æ£€ç´¢æ–‡æ¡£...")
        
        try:
            # è°ƒç”¨ RAG é“¾
            response = chain.invoke({"input": prompt})
            answer = response["answer"]
            
            # æ ¼å¼åŒ–ä¸€ä¸‹å‚è€ƒæ¥æº (å¯é€‰)
            sources_text = "\n\n**ğŸ“š å‚è€ƒæ–‡æ¡£ï¼š**\n"
            seen_sources = set()
            for doc in response["context"]:
                source_name = os.path.basename(doc.metadata.get('source', 'æœªçŸ¥'))
                if source_name not in seen_sources:
                    sources_text += f"- `{source_name}`\n"
                    seen_sources.add(source_name)
            
            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            full_response = answer + sources_text
            message_placeholder.markdown(full_response)
            
            # è®°å½•åŠ©æ‰‹å›å¤
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            message_placeholder.error(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            