import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

current_dir = os.path.dirname(os.path.abspath(__file__))
PERSIST_DIRECTORY = os.path.join(current_dir, "chroma_db")

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv("../.env") # è¯»å–ä¸Šä¸€çº§ç›®å½•çš„ .env

# æ£€æŸ¥ Key æ˜¯å¦å­˜åœ¨
if not os.getenv("GOOGLE_API_KEY"):
    print("âŒ é”™è¯¯: æœªæ‰¾åˆ° GOOGLE_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
    exit()

# æ•°æ®åº“å­˜å‚¨è·¯å¾„

def main():
    print(f"ğŸ“‚ æ•°æ®åº“å°†å­˜æ”¾åœ¨: {PERSIST_DIRECTORY}")

    # 2. åŠ è½½æ•°æ®ï¼šæ‰«æ data ç›®å½•ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶
    # glob="*.md" è¡¨ç¤ºåªçœ‹ markdown æ–‡ä»¶
    loader = DirectoryLoader('./data', glob="*.md", loader_cls=TextLoader)
    documents = loader.load()
    print(f"ğŸ“„ åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

    # 3. æ–‡æœ¬åˆ‡ç‰‡ (Chunking)
    # ä¸ºä»€ä¹ˆæ˜¯ 1000ï¼Ÿå› ä¸ºç»„ä»¶æ–‡æ¡£åŒ…å«è¡¨æ ¼å’Œé•¿ä»£ç ï¼Œåˆ‡å¤ªå°ä¼šæ–­ç« å–ä¹‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200, # é‡å éƒ¨åˆ†ï¼Œé˜²æ­¢åˆ‡æ–­å…³é”®ä¸Šä¸‹æ–‡
        separators=["\n## ", "\n### ", "\n", " ", ""] # ä¼˜å…ˆæŒ‰æ ‡é¢˜åˆ‡åˆ†
    )
    chunks = text_splitter.split_documents(documents)
    print(f"âœ‚ï¸ åˆ‡åˆ†æˆäº† {len(chunks)} ä¸ªç‰‡æ®µ")

    # 4. å‘é‡åŒ–å¹¶å­˜å‚¨ (Embedding & Storage)
    print("ğŸ’¾ æ­£åœ¨å­˜å…¥ ChromaDB (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")
    
    vector_store = Chroma.from_documents(
            documents=chunks,
            embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"), # ä½¿ç”¨ HuggingFace æ¨¡å‹
            persist_directory=PERSIST_DIRECTORY
        )
    
    
    # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ã€è®¡ç®—å‘é‡ã€å­˜å…¥æœ¬åœ°æ–‡ä»¶å¤¹
    
    print(f"âœ… æˆåŠŸï¼æ•°æ®åº“å·²ä¿å­˜åœ¨ {PERSIST_DIRECTORY}")

if __name__ == "__main__":
    main()