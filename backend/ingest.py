import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

current_dir = os.path.dirname(os.path.abspath(__file__))
PERSIST_DIRECTORY = os.path.join(current_dir, "chroma_db")

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv("../.env") # è¯»å–ä¸Šä¸€çº§ç›®å½•çš„ .env

# æ£€æŸ¥ Key æ˜¯å¦å­˜åœ¨
if not os.getenv("DEEPSEEK_API_KEY"):
    print("âŒ é”™è¯¯: æœªæ‰¾åˆ° DEEPSEEK_API_KEYï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
    exit()

# æ•°æ®åº“å­˜å‚¨è·¯å¾„

def main():
    print(f"ğŸ“‚ æ•°æ®åº“å°†å­˜æ”¾åœ¨: {PERSIST_DIRECTORY}")

    # 2. åŠ è½½æ•°æ®ï¼šæ‰«æ data ç›®å½•ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶
    # glob="*.md" è¡¨ç¤ºåªçœ‹ markdown æ–‡ä»¶
    loader = DirectoryLoader('./data', glob="*.md", loader_cls=TextLoader)
    documents = loader.load()
    print(f"ğŸ“„ åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

    md_header_splits = []
    
    for doc in documents:
        # å¯¹æ¯ä¸ªæ–‡æ¡£çš„å†…å®¹è¿›è¡Œæ ‡é¢˜åˆ‡åˆ†
        splits = markdown_splitter.split_text(doc.page_content)
        
        # ã€å…³é”®æ­¥éª¤ã€‘MarkdownSplitter åˆ‡å®Œåä¼šä¸¢å¤±åŸæ¥çš„ file_path (source)
        # æˆ‘ä»¬å¿…é¡»æ‰‹åŠ¨æŠŠåŸæ–‡æ¡£çš„ metadata (æ¯”å¦‚æ–‡ä»¶å) æ›´æ–°åˆ°æ–°åˆ‡ç‰‡é‡Œ
        for split in splits:
            split.metadata.update(doc.metadata)
            
        md_header_splits.extend(splits)

    print(f"ğŸ§© æŒ‰æ ‡é¢˜åˆ‡åˆ†åå¾—åˆ°äº† {len(md_header_splits)} ä¸ªè¯­ä¹‰ç‰‡æ®µ")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""] # äºŒæ¬¡åˆ‡åˆ†å°±ä¸éœ€è¦å†å…³æ³¨æ ‡é¢˜äº†ï¼Œä¸»è¦å…³æ³¨æ®µè½
    )
    
    final_chunks = text_splitter.split_documents(md_header_splits)
    print(f"âœ‚ï¸ æœ€ç»ˆåˆ‡åˆ†æˆäº† {len(final_chunks)} ä¸ªç‰‡æ®µ")

    # 4. å‘é‡åŒ–å¹¶å­˜å‚¨ (Embedding & Storage)
    print("ğŸ’¾ æ­£åœ¨å­˜å…¥ ChromaDB (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")
    
    vector_store = Chroma.from_documents(
            documents=final_chunks,
            embedding=HuggingFaceEmbeddings(model_name="BAAI/bge-small-zh-v1.5"), # ä½¿ç”¨ HuggingFace æ¨¡å‹
            persist_directory=PERSIST_DIRECTORY
        )
    
    
    # è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ã€è®¡ç®—å‘é‡ã€å­˜å…¥æœ¬åœ°æ–‡ä»¶å¤¹
    
    print(f"âœ… æˆåŠŸï¼æ•°æ®åº“å·²ä¿å­˜åœ¨ {PERSIST_DIRECTORY}")

if __name__ == "__main__":
    main()